{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pubg-light-gbm.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["YLjfKUYUovEA","5uibvhFXoCpm"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"wrdJKoryp4-M","colab_type":"text"},"cell_type":"markdown","source":["# PUBG finish placement prediction"]},{"metadata":{"id":"Z1NH5BmBpYAn","colab_type":"text"},"cell_type":"markdown","source":["## This is a fully functioning python notebook that reaches the MAE of 0.02. GridSearchCV and keras NN parts were excluded from execution due to RAM limitations. However, their code is presented in comments. "]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"L1LqRbu4peJz","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","import os\n","print(os.listdir(\"../input\"))\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","import gc, sys\n","gc.enable()\n","import time"],"execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5090264431f240aece2a223f9a3c832b4ef32b5c","id":"RrOBf1h9peJ2","colab_type":"text"},"cell_type":"markdown","source":["# Feature engineering"]},{"metadata":{"trusted":true,"_uuid":"be41aba596e0bfb966bad7b3973dfcd49182e7d6","id":"Qy-ZJooIpeJ4","colab_type":"code","colab":{}},"cell_type":"code","source":["INPUT_DIR = \"../input/\""],"execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61440ed3360902a4d910cfb1e28045b079cbd8e0","id":"bUKSy1UxpeJ9","colab_type":"code","colab":{}},"cell_type":"code","source":["# This function is required to handle the data feature editing. Otherwise, we will run out of RAM\n","# at least 16 GB of RAM is required\n","# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n","def reduce_mem_usage(df,display=False):\n","    \"\"\" iterate through all the columns of a dataframe and modify the data type\n","        to reduce memory usage.        \n","    \"\"\"\n","    start_time = time.time()\n","    if display:\n","        start_mem = df.memory_usage().sum() / 1024**2\n","        print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n","\n","    for col in df.columns:\n","        col_type = df[col].dtype\n","\n","        if col_type != object:\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","            if str(col_type)[:3] == 'int':\n","                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                    df[col] = df[col].astype(np.int8)\n","                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                    df[col] = df[col].astype(np.int16)\n","                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                    df[col] = df[col].astype(np.int32)\n","                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                    df[col] = df[col].astype(np.int64)  \n","            else:\n","                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","                    df[col] = df[col].astype(np.float16)\n","                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float64)\n","\n","    if display:\n","        end_mem = df.memory_usage().sum() / 1024**2\n","        print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n","        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n","        print('\\nTime elapsed %.0f sec\\n'%(time.time()-start_time))\n","\n","    return df"],"execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10b5ea79b5bd45980f6c7ff931aca359968ceed3","id":"KRxGs_OtpeJ_","colab_type":"code","colab":{}},"cell_type":"code","source":["# without feature engineering, our model will not be predicting very well\n","def feature_engineering(is_train=True,debug=True):\n","    \"\"\"\n","    Although a gamer can be very good at the game, \n","    but if other players of other groups in the same match is better than that player, he will still get lower score.\n","    This feature editing part takes into account each match groups' size and min, max, mean features. match size= how many people in the match  \n","    11 additional features are added based on game analysis.\n","    \"\"\"\n","    start_time = time.time()\n","    #test_idx = None\n","    if is_train: \n","        print(\"processing train.csv\")\n","        if debug == True:\n","            df = reduce_mem_usage( pd.read_csv('../input/train_V2.csv', nrows=10000), True)\n","        else:\n","            df = reduce_mem_usage(pd.read_csv('../input/train_V2.csv'), True)           \n","\n","        df = df[df['maxPlace'] > 1]\n","    else:\n","        print(\"processing test.csv\")\n","        df = reduce_mem_usage(pd.read_csv('../input/test_V2.csv'), True)\n","        #test_idx = df.Id\n","    \n","    print(\"remove some columns\")\n","    target = 'winPlacePerc'\n","\n","    print(\"Adding Features\")\n"," \n","    df['headshotrate'] = df['kills']/df['headshotKills']\n","    df['killStreakrate'] = df['killStreaks']/df['kills']\n","    df['healthitems'] = df['heals'] + df['boosts']\n","    df['totalDistance'] = df['rideDistance'] + df[\"walkDistance\"] + df[\"swimDistance\"]\n","    df['killPlace_over_maxPlace'] = df['killPlace'] / df['maxPlace']\n","    df['headshotKills_over_kills'] = df['headshotKills'] / df['kills']\n","    df['distance_over_weapons'] = df['totalDistance'] / df['weaponsAcquired']\n","    df['walkDistance_over_heals'] = df['walkDistance'] / df['heals']\n","    df['walkDistance_over_kills'] = df['walkDistance'] / df['kills']\n","    df['killsPerWalkDistance'] = df['kills'] / df['walkDistance']\n","    df[\"skill\"] = df[\"headshotKills\"] + df[\"roadKills\"]\n","\n","    df[df == np.Inf] = np.NaN\n","    df[df == np.NINF] = np.NaN\n","    \n","    print(\"Replacing Na's in DF\")\n","    df.fillna(0, inplace=True) # this is important for making group means, min,max.\n","    #gc.collect() # clean any residual unused var.s\n","    \n","    features = list(df.columns)\n","    features.remove(\"Id\")\n","    features.remove(\"matchId\")\n","    features.remove(\"groupId\")\n","    features.remove(\"matchType\")\n","    \n","    \n","    y = None\n","    \n","    \n","    if is_train: \n","        print(\"get target\")\n","        y = np.array(df.groupby(['matchId','groupId'])[target].agg('mean'), dtype=np.float64)\n","        features.remove(target)\n","\n","    print(\"get group mean feature\")\n","    agg = df.groupby(['matchId','groupId'])[features].agg('mean')\n","    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n","    \n","    if is_train: df_out = agg.reset_index()[['matchId','groupId']]\n","    else: df_out = df[['matchId','groupId']]\n","\n","    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n","    df_out = df_out.merge(agg_rank, suffixes=[\"_mean\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n","\n","    gc.collect()\n","    \n","    print(\"get group max feature\")\n","    agg = df.groupby(['matchId','groupId'])[features].agg('max')\n","    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n","    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n","    df_out = df_out.merge(agg_rank, suffixes=[\"_max\", \"_max_rank\"], how='left', on=['matchId', 'groupId'])\n","\n","    gc.collect()\n","    \n","    print(\"get group min feature\")\n","    agg = df.groupby(['matchId','groupId'])[features].agg('min')\n","    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n","    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n","    df_out = df_out.merge(agg_rank, suffixes=[\"_min\", \"_min_rank\"], how='left', on=['matchId', 'groupId'])\n","    \n","    #df_out = reduce_mem_usage(df_out)\n","    #del agg, agg_rank\n","    gc.collect()\n","    \n","    print(\"get group size feature\")\n","    agg = df.groupby(['matchId','groupId']).size().reset_index(name='group_size')\n","    df_out = df_out.merge(agg, how='left', on=['matchId', 'groupId'])\n","    \n","    #df_out = reduce_mem_usage(df_out)\n","    #del agg\n","    gc.collect()\n","    \n","    print(\"get match mean feature\")\n","    agg = df.groupby(['matchId'])[features].agg('mean').reset_index()\n","    df_out = df_out.merge(agg, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\n","    \n","    print(\"get match size feature\")\n","    agg = df.groupby(['matchId']).size().reset_index(name='match_size')\n","    df_out = df_out.merge(agg, how='left', on=['matchId'])\n","    \n","  \n","    df_out.drop([\"matchId\", \"groupId\"], axis=1, inplace=True)\n","    \n","    #feature_names = list(df_out.columns)\n","\n","    del df, agg, agg_rank\n","    gc.collect()\n","    print('\\nTime elapsed for feature engineering: %.0f sec'%(time.time()-start_time))\n","    return df_out, y #, feature_names, test_idx"],"execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a76be85df89e3522c9a52bc9ad38f7a7bb413c2","id":"lGZcksyPpeKC","colab_type":"code","colab":{}},"cell_type":"code","source":["# Process the training data :\n","x_train, y_train = feature_engineering(True, False)"],"execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2cd2169c1577f598e9bdf165c29f36a34178f043","id":"41EtUSOwpeKG","colab_type":"code","colab":{}},"cell_type":"code","source":["x_train = reduce_mem_usage(x_train, True)"],"execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40167ca3f1d7f830614a7b8ce4e2656e0de9b1f2","id":"eKS7KDVdpeKJ","colab_type":"code","colab":{}},"cell_type":"code","source":["print(x_train.shape)\n","x_train.head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Bn_Ir0ZsoNN2","colab_type":"code","colab":{}},"cell_type":"code","source":["y_train.shape"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YLjfKUYUovEA","colab_type":"text"},"cell_type":"markdown","source":["# GridSearchCV"]},{"metadata":{"id":"BFWfQEDGoRvb","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","# GridSearch parameters\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","gridParams = {\n","    'learning_rate': [ 0.03, 0.04, 0.05],\n","    'num_leaves': [ 33, 36, 39]\n","}\n","X_tr, X_test, y_tr, y_test = train_test_split(x_train, y_train, test_size=0.40, random_state=46)\n","del x_train, y_train\n","mdl = lgb.LGBMRegressor(metric='mae',\n","     objective=\"regression\", \n","    n_estimators=20000, \n","    bagging_fraction=0.7,\n","    bagging_seed=0, \n","    num_threads=4,\n","    colsample_bytree=0.7, \n","    num_boost_round=100)\n","\n","grid = GridSearchCV(mdl, gridParams, verbose=4, cv=None, n_jobs=-1, scoring='neg_mean_absolute_error')\n","'''"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0mtw-apao5nU","colab_type":"code","colab":{}},"cell_type":"code","source":["#grid.fit(X_test,y_test)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"m1EzeoIdpC87","colab_type":"code","colab":{}},"cell_type":"code","source":["#grid.best_params_"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vdw5K7vopMPR","colab_type":"code","colab":{}},"cell_type":"code","source":["#grid.best_score_"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5uibvhFXoCpm","colab_type":"text"},"cell_type":"markdown","source":["# Keras NN"]},{"metadata":{"trusted":true,"_uuid":"a83e803fe67f08c2e57549b89b7eebab21909ad5","id":"vccle9B3peKQ","colab_type":"code","colab":{}},"cell_type":"code","source":["# scaling does not help in gradient boosting algorithms most of the time!!!\n","# check performance without scaling \n","#from sklearn import preprocessing\n","# Scale the data to be in the range (-1 , 1)\n","#scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1), copy=False).fit(x_train)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"p5I5FIErlfk3","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","# For NN training, the following lines of code are required, then skip to submission part instead of training on GBM: \n","from sklearn.model_selection import train_test_split\n","# splitting data into test and train dataset\n","X_tr, X_test, y_tr, y_test = train_test_split(x_train, y_train, test_size=0.20,random_state=34)\n","del x_train, y_train\n","gc.collect()\n","\n","from sklearn import preprocessing\n","#1 Scale the data to be in the range (0 , 1)\n","scaler = preprocessing.QuantileTransformer().fit(X_tr)\n","X_tr=scaler.transform(X_tr)\n","X_test = scaler.transform(X_test)\n","'''"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BGXGNOWwm5FY","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n"," model = keras.Sequential([\n"," keras.layers.Dense(2048,kernel_initializer='he_normal', activation=tf.nn.relu, input_shape=(X_tr.shape[1],)), # neurons with relu activation, first layer with input \n"," keras.layers.BatchNormalization(),\n"," keras.layers.Dropout(0.3), # dropout for reducing the overfitting problem\n"," keras.layers.Dense(1024, kernel_initializer='he_normal', activation=tf.nn.relu), # 2nd hidden layer\n"," keras.layers.BatchNormalization(),   \n"," keras.layers.Dropout(0.3),\n"," keras.layers.Dense(512,kernel_initializer='he_normal', activation=tf.nn.relu), # 3rd hidden layer\n"," keras.layers.BatchNormalization(),\n"," keras.layers.Dropout(0.3),\n"," keras.layers.Dense(256,kernel_initializer='he_normal', activation=tf.nn.relu),\n"," keras.layers.BatchNormalization(),\n"," keras.layers.Dropout(0.3),   \n"," keras.layers.Dense(64, kernel_initializer='he_normal', activation=tf.nn.relu),\n"," keras.layers.BatchNormalization(),\n"," keras.layers.Dropout(0.3),   \n"," keras.layers.Dense(1, kernel_initializer='normal', activation='sigmoid')]) #  output layer \n","\n","model.compile(loss='mse', #this loss method is useful for numeric prediction\n"," optimizer=tf.train.AdamOptimizer(learning_rate=0.001), metrics=['mae'])\n","model.summary()\n","history1 = model.fit(X_tr, y_tr, epochs = 200, batch_size = 20480, verbose=1, validation_data = (X_test, y_test))\n","'''"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_dk5lsDynDkA","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","df_test = pd.read_csv('../input/test_V2.csv')\n","df_test = feature_engineering(False, False)\n","df_test = scaler.transform(df_test)\n","preds = model.predict(df_test)\n","'''"],"execution_count":0,"outputs":[]},{"metadata":{"_uuid":"35fc6a6f1cd65c1efdc184c5c5c0cd19a44e02ef","id":"X6_Xlbn6peKV","colab_type":"text"},"cell_type":"markdown","source":["\n","# Light GBM model"]},{"metadata":{"trusted":true,"_uuid":"82c731c9a88bf0cd2925afc34e51ffb7aeb39d72","id":"PeItmxrlpeKY","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","# splitting data into test and train dataset\n","X_tr, X_test, y_tr, y_test = train_test_split(x_train, y_train, test_size=0.10, random_state=46)\n","del x_train, y_train\n","gc.collect()"],"execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10cbac15ba496177aa3cc755eed3b6db6118df3f","id":"7hJP4F0upeKc","colab_type":"code","colab":{}},"cell_type":"code","source":["# the most important part is to tune the parameters of the model correctly. GridSearchCV is a good start\n","import lightgbm as lgb\n","\n","parameters = {\"objective\" : \"regression\", \"metric\" : \"mae\", 'n_estimators':20000, 'early_stopping_rounds':350,\n","              \"num_leaves\" : 35, \"learning_rate\" : 0.03, \"bagging_fraction\" : 0.7,\n","               \"bagging_seed\" : 0, \"num_threads\" : 4,\"colsample_bytree\" : 0.7, 'num_boost_round': 60000\n","             } "],"execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1036e3690180111e0fb5759ccccc84db669a004b","id":"Jvt_0uK0peKe","colab_type":"code","colab":{}},"cell_type":"code","source":["# training the model\n","train_data = lgb.Dataset(X_tr, label=y_tr)\n","test_data = lgb.Dataset(X_test, label=y_test)\n","del X_tr, X_test, y_tr, y_test\n","gc.collect()"],"execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9b14df3d48b163c283816dc036a073b8f0375d8","id":"JGTqpTJzpeKh","colab_type":"code","colab":{}},"cell_type":"code","source":["model = lgb.train(parameters, train_set = train_data, valid_sets=[train_data, test_data], verbose_eval=500) \n","# overfitting does not happen as long as we keep num_leaves low\n","#0.0312363\n","#0.0307479"],"execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0174ad79e59a4568f45eb9c97642598c9803f2ca","id":"fpP3yK6apeKk","colab_type":"code","colab":{}},"cell_type":"code","source":["del test_data, train_data\n","gc.collect()"],"execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db18e655782b59a5a1a07026594c3d645b5188b5","id":"OIgwpoJOpeKm","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","# test data editing and scaling\n","x_test, _ = feature_engineering(False, False)\n","x_test = reduce_mem_usage(x_test, True)"],"execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec48df78cc6a9cc1ff5cc307d4cf124adb294ef6","id":"9Vt8i4CypeKp","colab_type":"code","colab":{}},"cell_type":"code","source":["# predicting test dataset\n","preds = model.predict(x_test, num_iteration = model.best_iteration)\n","del x_test\n","gc.collect()"],"execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af4c50dca242a88ba07991e173b303535f80810f","id":"fqS-FqM9peKs","colab_type":"code","colab":{}},"cell_type":"code","source":["preds"],"execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fd0f7c591b0275a97b0614250825a8a8841621b","id":"9DbkZkTZpeK3","colab_type":"code","colab":{}},"cell_type":"code","source":["# making the predictions in [0, 1] range\n","preds = preds.reshape(-1)\n","#preds[preds > 1] = 1\n","#preds[preds < 0] = 0"],"execution_count":0,"outputs":[]},{"metadata":{"id":"a-6QeO3Bnrem","colab_type":"text"},"cell_type":"markdown","source":["# Submission"]},{"metadata":{"trusted":true,"_uuid":"cc547914e9a7342faf4dac404a5c75b2a4f5ca7d","id":"7HCo0X-ppeK5","colab_type":"code","colab":{}},"cell_type":"code","source":["df_sub = pd.read_csv(INPUT_DIR + 'test_V2.csv')"],"execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1c33f1723389256ca1e89fe36720715517e95f8","id":"NeM-86APpeK7","colab_type":"code","colab":{}},"cell_type":"code","source":["# Submission dealing with edge cases based on MaxPlace\n","# Credits: https://www.kaggle.com/anycode/simple-nn-baseline-4\n","df_sub['winPlacePerc'] = preds\n","df_sub.loc[df_sub.maxPlace == 0, \"winPlacePerc\"] = 0\n","df_sub.loc[df_sub.maxPlace == 1, \"winPlacePerc\"] = 1\n","subset = df_sub.loc[df_sub.maxPlace > 1]\n","gap = 1.0 / (subset.maxPlace.values - 1)\n","new_perc = np.around(subset.winPlacePerc.values / gap) * gap\n","df_sub.loc[df_sub.maxPlace > 1, \"winPlacePerc\"] = new_perc\n","\n","df_sub.loc[(df_sub.maxPlace > 1) & (df_sub.numGroups == 1), \"winPlacePerc\"] = 0\n","assert df_sub[\"winPlacePerc\"].isnull().sum() == 0"],"execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50d887830447706bbb652a62613192cc03097f93","id":"2PMOwfmMpeK-","colab_type":"code","colab":{}},"cell_type":"code","source":["df_sub.head()"],"execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40192c19381471ae94233d0cb24e113206aca3c8","id":"s-O63h8rpeLC","colab_type":"code","colab":{}},"cell_type":"code","source":["my_submission = pd.DataFrame({'Id': df_sub.Id, 'winPlacePerc': df_sub['winPlacePerc']})\n","my_submission.to_csv('submission1.csv', index=False)"],"execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02b32cebe8902d9363cf3af364cf6893277a4c8a","id":"mM26Bvs8peLF","colab_type":"code","colab":{}},"cell_type":"code","source":["my_submission['winPlacePerc'].head(10)"],"execution_count":0,"outputs":[]}]}